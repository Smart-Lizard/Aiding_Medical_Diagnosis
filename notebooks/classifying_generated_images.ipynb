{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fpFxmN92IRZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision medmnist\n",
        "!pip install git+https://github.com/MedMNIST/MedMNIST.git"
      ],
      "metadata": {
        "id": "KDab1a-2IZ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install scikit-learn==1.3.0\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import medmnist\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from medmnist import INFO, PathMNIST\n",
        "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
        "import seaborn as sns\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "erVI7AETIeGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "all_true_labels_by_step_label = defaultdict(lambda: defaultdict(list))  # Store true labels for each step and label\n",
        "all_predicted_labels_by_step_label = defaultdict(lambda: defaultdict(list))  # Store predicted labels for each step and label\n",
        "\n",
        "# Load and preprocess dataset (224x224 resolution)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n",
        "])\n",
        "\n",
        "# Define the Model with ResNet18 pretrained\n",
        "class PathMNISTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=9):  # PathMNIST has 9 classes\n",
        "        super(PathMNISTClassifier, self).__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Modify the final fully connected layer to match the number of classes in PathMNIST\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.data[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Function to unnormalize an image\n",
        "def unnormalize(tensor, mean, std):\n",
        "    \"\"\"Unnormalizes a tensor image with mean and standard deviation.\"\"\"\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "JVVknvQvIemt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify subfolders and main folder\n",
        "subfolders_to_classify = [\n",
        "    \"a histopathological image of an area with adipose\",\n",
        "    \"a histopathological image of an area with mucus\",\n",
        "    \"a histopathological image of an area with cancer-associated stroma\",\n",
        "    \"a histopathological image of an area with smooth muscle\",\n",
        "    \"a histopathological image of an area with colorectal adenocarcinoma epithelium\",\n",
        "    \"a histopathological image of an area with lymphocytes\",\n",
        "    \"a histopathological image of an area with debris\",\n",
        "    \"a histopathological image of an area with background\",\n",
        "    \"a histopathological image of an area with normal colon mucosa\"\n",
        "    ]\n",
        "\n",
        "main_folder = \"/content/drive/MyDrive/wandb_images/pmnist_v10_600checkpoint\"  # Replace with your main folder path\n",
        "\n",
        "# Load the best model\n",
        "model_path = '/content/drive/MyDrive/pathmnist/saved_trained_models/224_classification_model.pth'\n",
        "model = PathMNISTClassifier(num_classes=9)\n",
        "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "# Get label names\n",
        "info = INFO['pathmnist']\n",
        "label_names = list(info['label'].values())\n",
        "enable_display = True\n",
        "\n",
        "label_prefix = \"a histopathological image of an area with \""
      ],
      "metadata": {
        "id": "bHMuaoGgqzBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap  # Import textwrap for wrapping titles\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# Initialize a dictionary to store results for each subfolder\n",
        "subfolder_results = {}\n",
        "\n",
        "# run the below classification code only for specific training step or all\n",
        "SUB_SUBFOLDER_NAME_CRITERIA = \"\"  # run over all steps\n",
        "# SUB_SUBFOLDER_NAME_CRITERIA = \"step1131\"  # run for only this step\n",
        "\n",
        "font_prop = fm.FontProperties(weight='bold')\n",
        "\n",
        "# Iterate through subfolders and classify images\n",
        "for subfolder_name in subfolders_to_classify:\n",
        "    subfolder_path = os.path.join(main_folder, subfolder_name)\n",
        "\n",
        "    # Check if the subfolder exists before proceeding\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(f\"Skipping subfolder '{subfolder_name}' as it does not exist.\")\n",
        "        continue  # Skip to the next subfolder\n",
        "\n",
        "    # Extract the true label by removing the prefix\n",
        "    true_label = subfolder_name.replace(label_prefix, \"\")\n",
        "\n",
        "    # Initialize a list to store results for this subfolder\n",
        "    subfolder_results[subfolder_name] = []\n",
        "\n",
        "    # Iterate through sub-subfolders within the current subfolder\n",
        "    for sub_subfolder_name in os.listdir(subfolder_path):\n",
        "        sub_subfolder_path = os.path.join(subfolder_path, sub_subfolder_name)\n",
        "\n",
        "        # Skip if criteria is defined and doesn't match\n",
        "        if SUB_SUBFOLDER_NAME_CRITERIA and SUB_SUBFOLDER_NAME_CRITERIA not in sub_subfolder_name:\n",
        "            continue  # Skip to the next iteration\n",
        "\n",
        "        # Check if it's a directory\n",
        "        if os.path.isdir(sub_subfolder_path):\n",
        "\n",
        "            # Initialize counters for true positives, false negatives, and false positives\n",
        "            true_positives = 0\n",
        "            false_negatives = 0\n",
        "            fp_counts = {label: 0 for label in label_names}\n",
        "\n",
        "            testsaved_data = []\n",
        "            image_names = []\n",
        "\n",
        "            # Iterate through images within the sub-subfolder\n",
        "            for image_name in os.listdir(sub_subfolder_path):\n",
        "                if image_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    image_path = os.path.join(sub_subfolder_path, image_name)\n",
        "                    image = Image.open(image_path).convert('RGB')\n",
        "                    testsaved_data.append((image, 0))\n",
        "                    image_names.append(image_name)\n",
        "\n",
        "            testsaved_loader = DataLoader(CustomDataset(testsaved_data, transform=transform), batch_size=1)\n",
        "            print(f\"\\nFolder: {subfolder_name}/{sub_subfolder_name}\")\n",
        "\n",
        "            # Initialize figure and axes for the grid\n",
        "            num_images = len(testsaved_loader)  # Get the total number of images\n",
        "            num_rows = (num_images + 4) // 5  # Calculate the number of rows needed\n",
        "            fig, axes = plt.subplots(num_rows, 5, figsize=(10, num_rows * 3))  # Adjust figsize as needed\n",
        "            # Flatten the axes array for easier indexing\n",
        "            axes = axes.flatten()\n",
        "\n",
        "            # Classify images in the current sub-subfolder\n",
        "            for i, (saved_image, _) in enumerate(testsaved_loader):\n",
        "                saved_output = model(saved_image)\n",
        "                _, saved_predicted = torch.max(saved_output, 1)\n",
        "                predicted_label_name = label_names[saved_predicted.item()]\n",
        "\n",
        "                # Compare predicted label with true label\n",
        "                if predicted_label_name == true_label:\n",
        "                    true_positives += 1\n",
        "                else:\n",
        "                    false_negatives += 1\n",
        "                    # Increment FP count for the predicted label\n",
        "                    fp_counts[predicted_label_name] += 1\n",
        "\n",
        "                # print(f\"   Image: {image_names[i]}, Predicted Label: {predicted_label_name} ({saved_predicted.item()})\")\n",
        "                # Unnormalize and display the image\n",
        "                if enable_display:\n",
        "                    mean = [0.485, 0.456, 0.406]\n",
        "                    std = [0.229, 0.224, 0.225]\n",
        "                    unnormalized_image = unnormalize(saved_image.clone().squeeze(), mean, std)\n",
        "                    axes[i].imshow(unnormalized_image.permute(1, 2, 0).clip(0, 1))\n",
        "                    axes[i].axis('off')\n",
        "                   # axes[i].set_title(f\"Predicted: {predicted_label_name}\")  # Add title if desired\n",
        "                    title = f\"Predicted: {predicted_label_name}\"\n",
        "                    wrapped_title = \"\\n\".join(textwrap.wrap(title, width=10))  # Adjust width as needed\n",
        "                    axes[i].set_title(wrapped_title, fontsize=9, y=1.0, pad=0, fontproperties=font_prop)  # Adjust fontsize and pad\n",
        "\n",
        "            if enable_display:\n",
        "              # Hide any unused subplots\n",
        "              for j in range(num_images, num_rows * 5):\n",
        "                  axes[j].axis('off')\n",
        "\n",
        "              # Display the grid\n",
        "              plt.subplots_adjust(hspace=0.8)\n",
        "              plt.tight_layout()\n",
        "              plt.show()\n",
        "\n",
        "            # Print results for the current sub-subfolder\n",
        "            print(f\"Result:\")\n",
        "            print(f\" {sub_subfolder_name} True Positives: {true_positives} False Negatives: {false_negatives}\\n\")\n",
        "            # Print false positives for this subfolder\n",
        "            print(f\"False Positives:\")\n",
        "            for label, count in fp_counts.items():\n",
        "                if count > 0:  # Only print labels with false positives\n",
        "                    print(f\"  {label}: {count}\")\n",
        "            # Append results for this sub-subfolder to the list for this subfolder\n",
        "            subfolder_results[subfolder_name].append((sub_subfolder_name, true_positives, false_negatives,fp_counts))\n"
      ],
      "metadata": {
        "id": "RGcP7sSlW1I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_counts_by_label_step = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "print(f\"Final counts of False Positives:\")\n",
        "for subfolder_name, results in subfolder_results.items():\n",
        "    true_label = subfolder_name.replace(label_prefix, \"\")  # Extract true label\n",
        "    for step, true_positives, false_negatives, fp_counts in results:\n",
        "        for predicted_label, count in fp_counts.items():\n",
        "            if predicted_label != true_label:  # Count as FP if predicted label is not the true label\n",
        "                fp_counts_by_label_step[predicted_label][step] += count\n",
        "\n",
        "for label, step_counts in fp_counts_by_label_step.items():\n",
        "    print(f\"Label: {label}\")\n",
        "    for step, count in step_counts.items():\n",
        "        print(f\"  Step: {step:<10} FP: {count}\")"
      ],
      "metadata": {
        "id": "hvHHOF7gRAMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate precision, recall, and F1-score\n",
        "def calculate_metrics(tp, fp, fn):\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "aggregate_metrics_by_step = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})\n",
        "\n",
        "print(f\"METRICS:\")\n",
        "# Iterate through the subfolder results\n",
        "for subfolder_name, results in subfolder_results.items():\n",
        "    # Get the true label from the subfolder name\n",
        "    true_label = subfolder_name.replace(label_prefix, \"\")\n",
        "\n",
        "    # Print label\n",
        "    print(f\"Label: {true_label}\")\n",
        "\n",
        "    # Initialize FP count for this subfolder\n",
        "    #fp_counts_for_subfolder = {label: 0 for label in subfolders_to_classify}  # Reset FP count\n",
        "    for step, true_positives, false_negatives,_ in results:\n",
        "        # Calculate local false positives for the current step\n",
        "        local_fp = fp_counts_by_label_step[true_label][step]\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision, recall, f1_score = calculate_metrics(true_positives, local_fp, false_negatives)\n",
        "        # collect aggregates\n",
        "        aggregate_metrics_by_step[step]['tp'] += true_positives\n",
        "        aggregate_metrics_by_step[step]['fp'] += local_fp\n",
        "        aggregate_metrics_by_step[step]['fn'] += false_negatives\n",
        "\n",
        "        # Print the result for the current step\n",
        "        print(\n",
        "            f\" {step:<8} \"\n",
        "            f\"P: {precision:.4f} R: {recall:.4f} \"\n",
        "            f\"F1: {f1_score:.4f} TP:{true_positives} FP:{local_fp} FN:{false_negatives}\"\n",
        "        )\n",
        "\n",
        "print(\"\\nAGGREGATE METRICS:\")\n",
        "for step, metrics in aggregate_metrics_by_step.items():\n",
        "    precision, recall, f1_score = calculate_metrics(metrics['tp'], metrics['fp'], metrics['fn'])\n",
        "    print(f\" {step:<8} \"\n",
        "          f\"P: {precision:.4f} R: {recall:.4f} \"\n",
        "          f\"F1: {f1_score:.4f} TP:{metrics['tp']} FP:{metrics['fp']} FN:{metrics['fn']}\")"
      ],
      "metadata": {
        "id": "ytuZt0piYmwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}