{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fpFxmN92IRZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision medmnist\n",
        "!pip install git+https://github.com/MedMNIST/MedMNIST.git"
      ],
      "metadata": {
        "id": "KDab1a-2IZ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install scikit-learn==1.3.0\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import medmnist\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from medmnist import INFO, PathMNIST\n",
        "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
        "import seaborn as sns\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "erVI7AETIeGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "all_true_labels_by_step_label = defaultdict(lambda: defaultdict(list))  # Store true labels for each step and label\n",
        "all_predicted_labels_by_step_label = defaultdict(lambda: defaultdict(list))  # Store predicted labels for each step and label\n",
        "\n",
        "# Load and preprocess dataset (224x224 resolution)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n",
        "])\n",
        "\n",
        "# Define the Model with ResNet18 pretrained\n",
        "class PathMNISTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=9):  # PathMNIST has 9 classes\n",
        "        super(PathMNISTClassifier, self).__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Modify the final fully connected layer to match the number of classes in PathMNIST\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.data[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Function to unnormalize an image\n",
        "def unnormalize(tensor, mean, std):\n",
        "    \"\"\"Unnormalizes a tensor image with mean and standard deviation.\"\"\"\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "JVVknvQvIemt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify subfolders and main folder\n",
        "subfolders_to_classify = [\n",
        "    \"a histopathological image of an area with adipose\",\n",
        "    \"a histopathological image of an area with mucus\",\n",
        "    \"a histopathological image of an area with cancer-associated stroma\",\n",
        "    \"a histopathological image of an area with smooth muscle\",\n",
        "    \"a histopathological image of an area with colorectal adenocarcinoma epithelium\",\n",
        "    \"a histopathological image of an area with lymphocytes\",\n",
        "    \"a histopathological image of an area with debris\",\n",
        "    \"a histopathological image of an area with background\",\n",
        "    \"a histopathological image of an area with normal colon mucosa\"\n",
        "    ]\n",
        "\n",
        "main_folder = \"/content/drive/MyDrive/pathmnist/images\"  # Replace with your main folder path\n",
        "\n",
        "# Load the best model\n",
        "model_path = '/content/drive/MyDrive/pathmnist/saved_trained_models/224_classification_model.pth'\n",
        "model = PathMNISTClassifier(num_classes=9)\n",
        "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "# Get label names\n",
        "info = INFO['pathmnist']\n",
        "label_names = list(info['label'].values())\n",
        "enable_display = True\n",
        "\n",
        "label_prefix = \"a histopathological image of an area with \""
      ],
      "metadata": {
        "id": "bHMuaoGgqzBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import textwrap  # Import textwrap for wrapping titles\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# Initialize a dictionary to store results for each subfolder\n",
        "subfolder_results = {}\n",
        "\n",
        "# run the below classification code only for specific training step or all\n",
        "SUB_SUBFOLDER_NAME_CRITERIA = \"\"  # run over all steps\n",
        "# SUB_SUBFOLDER_NAME_CRITERIA = \"step1131\"  # run for only this step\n",
        "\n",
        "font_prop = fm.FontProperties(weight='bold')\n",
        "\n",
        "# Iterate through subfolders and classify images\n",
        "for subfolder_name in subfolders_to_classify:\n",
        "    subfolder_path = os.path.join(main_folder, subfolder_name)\n",
        "\n",
        "    # Check if the subfolder exists before proceeding\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(f\"Skipping subfolder '{subfolder_name}' as it does not exist.\")\n",
        "        continue  # Skip to the next subfolder\n",
        "\n",
        "    # Extract the true label by removing the prefix\n",
        "    true_label = subfolder_name.replace(label_prefix, \"\")\n",
        "\n",
        "    # Initialize a list to store results for this subfolder\n",
        "    subfolder_results[subfolder_name] = []\n",
        "\n",
        "    # Get a list of all image files in the subfolder\n",
        "    all_image_files = [f for f in os.listdir(subfolder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    # Randomly select 10 images\n",
        "    selected_images = random.sample(all_image_files, min(100, len(all_image_files)))  # Select 10 or fewer if less than 10 images are present\n",
        "\n",
        "    # Initialize counters for true positives, false negatives, and false positives\n",
        "    true_positives = 0\n",
        "    false_negatives = 0\n",
        "    fp_counts = {label: 0 for label in label_names}\n",
        "\n",
        "    testsaved_data = []\n",
        "    image_names = []\n",
        "\n",
        "    # Classify the selected images\n",
        "    for image_name in selected_images:\n",
        "        image_path = os.path.join(subfolder_path, image_name)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        testsaved_data.append((image, 0))\n",
        "        image_names.append(image_name)\n",
        "\n",
        "    testsaved_loader = DataLoader(CustomDataset(testsaved_data, transform=transform), batch_size=1)\n",
        "    print(f\"\\nFolder: {subfolder_name}\")\n",
        "\n",
        "    # Initialize figure and axes for the grid\n",
        "    num_images = len(testsaved_loader)  # Get the total number of images\n",
        "    num_rows = (num_images + 4) // 5  # Calculate the number of rows needed\n",
        "    fig, axes = plt.subplots(num_rows, 5, figsize=(10, num_rows * 3))  # Adjust figsize as needed\n",
        "    # Flatten the axes array for easier indexing\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Classify images in the current sub-subfolder\n",
        "    for i, (saved_image, _) in enumerate(testsaved_loader):\n",
        "        saved_output = model(saved_image)\n",
        "        _, saved_predicted = torch.max(saved_output, 1)\n",
        "        predicted_label_name = label_names[saved_predicted.item()]\n",
        "\n",
        "        # Compare predicted label with true label\n",
        "        if predicted_label_name == true_label:\n",
        "            true_positives += 1\n",
        "        else:\n",
        "            false_negatives += 1\n",
        "            # Increment FP count for the predicted label\n",
        "            fp_counts[predicted_label_name] += 1\n",
        "\n",
        "        # print(f\"   Image: {image_names[i]}, Predicted Label: {predicted_label_name} ({saved_predicted.item()})\")\n",
        "        # Unnormalize and display the image\n",
        "        if enable_display:\n",
        "            mean = [0.485, 0.456, 0.406]\n",
        "            std = [0.229, 0.224, 0.225]\n",
        "            unnormalized_image = unnormalize(saved_image.clone().squeeze(), mean, std)\n",
        "            axes[i].imshow(unnormalized_image.permute(1, 2, 0).clip(0, 1))\n",
        "            axes[i].axis('off')\n",
        "            # axes[i].set_title(f\"Predicted: {predicted_label_name}\")  # Add title if desired\n",
        "            title = f\"Predicted: {predicted_label_name}\"\n",
        "            wrapped_title = \"\\n\".join(textwrap.wrap(title, width=10))  # Adjust width as needed\n",
        "            axes[i].set_title(wrapped_title, fontsize=9, y=1.0, pad=0, fontproperties=font_prop)  # Adjust fontsize and pad\n",
        "\n",
        "    if enable_display:\n",
        "      # Hide any unused subplots\n",
        "      for j in range(num_images, num_rows * 5):\n",
        "          axes[j].axis('off')\n",
        "\n",
        "      # Display the grid\n",
        "      plt.subplots_adjust(hspace=0.8)\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "    # Print results for the current sub-subfolder\n",
        "    print(f\"Result:\")\n",
        "    # Since you're not iterating through sub-subfolders, use subfolder_name instead of sub_subfolder_name\n",
        "    print(f\" {subfolder_name} True Positives: {true_positives} False Negatives: {false_negatives}\\n\")\n",
        "    # Print false positives for this subfolder\n",
        "    print(f\"False Positives:\")\n",
        "    for label, count in fp_counts.items():\n",
        "        if count > 0:  # Only print labels with false positives\n",
        "            print(f\"  {label}: {count}\")\n",
        "    # Append results for this sub-subfolder to the list for this subfolder\n",
        "    # Use subfolder_name as the step since there are no sub-subfolders\n",
        "    subfolder_results[subfolder_name].append((subfolder_name, true_positives, false_negatives,fp_counts))"
      ],
      "metadata": {
        "id": "RGcP7sSlW1I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_counts_by_label = defaultdict(int)  # Stores FP counts for each label\n",
        "\n",
        "print(f\"Final counts of False Positives:\")\n",
        "for subfolder_name, results in subfolder_results.items():\n",
        "    true_label = subfolder_name.replace(label_prefix, \"\")  # Extract true label\n",
        "    # Since there's only one step (the subfolder itself), get the results directly\n",
        "    _, true_positives, false_negatives, fp_counts = results[0]\n",
        "\n",
        "    for predicted_label, count in fp_counts.items():\n",
        "        if predicted_label != true_label:  # Count as FP if predicted label is not the true label\n",
        "            fp_counts_by_label[predicted_label] += count  # Accumulate FP counts for the label\n",
        "\n",
        "# Print results\n",
        "for label, count in fp_counts_by_label.items():\n",
        "    print(f\"Label: {label}, FP: {count}\")"
      ],
      "metadata": {
        "id": "hvHHOF7gRAMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate precision, recall, and F1-score\n",
        "def calculate_metrics(tp, fp, fn):\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "aggregate_metrics = {'tp': 0, 'fp': 0, 'fn': 0}  # Aggregate metrics for all labels\n",
        "\n",
        "print(f\"METRICS:\")\n",
        "for subfolder_name, results in subfolder_results.items():\n",
        "    true_label = subfolder_name.replace(label_prefix, \"\")  # Extract true label\n",
        "    print(f\"Label: {true_label}\")\n",
        "\n",
        "    # Get results directly (no step iteration)\n",
        "    _, true_positives, false_negatives, _ = results[0]\n",
        "    local_fp = fp_counts_by_label[true_label]  # Get FP for this label\n",
        "\n",
        "    precision, recall, f1_score = calculate_metrics(true_positives, local_fp, false_negatives)\n",
        "\n",
        "    # Update aggregate metrics\n",
        "    aggregate_metrics['tp'] += true_positives\n",
        "    aggregate_metrics['fp'] += local_fp\n",
        "    aggregate_metrics['fn'] += false_negatives\n",
        "\n",
        "    # Print results (no step information)\n",
        "    print(f\" P: {precision:.4f} R: {recall:.4f} F1: {f1_score:.4f} TP:{true_positives} FP:{local_fp} FN:{false_negatives}\")\n",
        "\n",
        "print(\"\\nAGGREGATE METRICS:\")\n",
        "precision, recall, f1_score = calculate_metrics(aggregate_metrics['tp'], aggregate_metrics['fp'], aggregate_metrics['fn'])\n",
        "print(f\" P: {precision:.4f} R: {recall:.4f} F1: {f1_score:.4f} TP:{aggregate_metrics['tp']} FP:{aggregate_metrics['fp']} FN:{aggregate_metrics['fn']}\")"
      ],
      "metadata": {
        "id": "ytuZt0piYmwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}